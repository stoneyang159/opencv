{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd08c0a916babf24023fbb7aa79c140667f707130a9df3956319c8a02c18f7513a2",
   "display_name": "Python 3.8.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConvolutionalResnet18(models.ResNet):\n",
    "    def __init__(self, num_classes=1000, pretrained=False, **kwargs):\n",
    "\n",
    "        # Start with standard resnet18 defined here \n",
    "        super().__init__(block = models.resnet.BasicBlock, layers = [2, 2, 2, 2], num_classes = num_classes, **kwargs)\n",
    "        if pretrained:\n",
    "            state_dict = load_state_dict_from_url( models.resnet.model_urls[\"resnet18\"], progress=True)\n",
    "            self.load_state_dict(state_dict)\n",
    "\n",
    "        # Replace AdaptiveAvgPool2d with standard AvgPool2d \n",
    "        self.avgpool = nn.AvgPool2d((7, 7))\n",
    "\n",
    "        # Convert the original fc layer to a convolutional layer.  \n",
    "        self.last_conv = torch.nn.Conv2d( in_channels = self.fc.in_features, out_channels = num_classes, kernel_size = 1)\n",
    "        self.last_conv.weight.data.copy_( self.fc.weight.data.view ( *self.fc.weight.data.shape, 1, 1))\n",
    "        self.last_conv.bias.data.copy_ (self.fc.bias.data)\n",
    "\n",
    "    # Reimplementing forward pass. \n",
    "    def _forward_impl(self, x):\n",
    "        # Standard forward for resnet18\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        # Notice, there is no forward pass \n",
    "        # through the original fully connected layer. \n",
    "        # Instead, we forward pass through the last conv layer\n",
    "        x = self.last_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torchvision.models.resnet.ResNet"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "models.ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['tench, Tinca tinca',\n",
       " 'goldfish, Carassius auratus',\n",
       " 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
       " 'tiger shark, Galeocerdo cuvieri',\n",
       " 'hammerhead, hammerhead shark',\n",
       " 'electric ray, crampfish, numbfish, torpedo',\n",
       " 'stingray',\n",
       " 'cock',\n",
       " 'hen',\n",
       " 'ostrich, Struthio camelus']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image\n",
    "original_image = cv2.imread('/data/file/img/camel.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(725, 1920, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "original_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert original image to RGB format\n",
    "image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(725, 1920, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform input image\n",
    "# convert to Tensor\n",
    "# subtract mean\n",
    "# divide by standard deviation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.225],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "image = transform(image)\n",
    "image = image.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/roczhang/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
      "100%|██████████| 44.7M/44.7M [00:30<00:00, 1.54MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = FullyConvolutionalResnet18(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Response map shape: torch.Size([1, 1000, 3, 8])\ntensor([[[978, 557, 557, 557, 557, 557, 354, 682],\n         [978, 970, 980, 977, 858, 970, 354, 461],\n         [141, 143, 977, 977, 977, 977, 354, 354]]])\nPredicted Class: Arabian camel, dromedary, Camelus dromedarius tensor([354])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    preds = model(image)\n",
    "    preds = torch.softmax(preds, dim=1)\n",
    "\n",
    "    print('Response map shape:', preds.shape)\n",
    "\n",
    "    pred, class_idx = torch.max(preds, dim=1)\n",
    "    print(class_idx)\n",
    "\n",
    "    row_max, row_idx = torch.max(pred, dim=1)\n",
    "    col_max, col_idx = torch.max(row_max, dim=1)\n",
    "    predicted_class = class_idx[0, row_idx[0, col_idx], col_idx]\n",
    "\n",
    "    print('Predicted Class:', labels[predicted_class], predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_map = preds[0, predicted_class, :, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_map = score_map[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_map = cv2.resize(score_map, (original_image.shape[1], original_image.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(725, 1920)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "score_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, score_map_for_contours = cv2.threshold(score_map, 0.25, 1, type=cv2.THRESH_BINARY)\n",
    "score_map_for_contours = score_map_for_contours.astype(np.uint8).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours, _ = cv2.findContours(score_map_for_contours, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rect = cv2.boundingRect(contours[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1384, 186, 536, 539)"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_map = score_map - np.min(score_map[:])\n",
    "score_map = score_map / np.max(score_map[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [  2,   2,   1],\n",
       "        [  2,   2,   1],\n",
       "        [  2,   2,   1]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [  2,   2,   1],\n",
       "        [  2,   2,   1],\n",
       "        [  2,   2,   1]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [  2,   2,   1],\n",
       "        [  2,   2,   1],\n",
       "        [  2,   2,   1]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [ 46,  61,  79],\n",
       "        [ 68,  83, 102],\n",
       "        [  0,   0, 255]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [ 55,  70,  88],\n",
       "        [ 31,  46,  65],\n",
       "        [  0,   0, 255]],\n",
       "\n",
       "       [[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        ...,\n",
       "        [  0,   0, 255],\n",
       "        [  0,   0, 255],\n",
       "        [  0,   0, 255]]], dtype=uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "score_map = cv2.cvtColor(score_map, cv2.COLOR_GRAY2BGR)\n",
    "masked_image = (original_image * score_map).astype(np.uint8)\n",
    "cv2.rectangle(masked_image, rect[:2], (rect[0] + rect[2], rect[1] + rect[3]), (0, 0, 255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Original Image\", original_image)\n",
    "cv2.imshow(\"scaled_score_map\", score_map)\n",
    "cv2.imshow(\"activations_and_bbox\", masked_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}